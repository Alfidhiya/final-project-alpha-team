{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting Customer Churn in Telecom: A Machine Learning Approach\n",
        "By Hans Darmawan - JCDS2602\n",
        "\n",
        "---\n",
        "## Section 1. Business Understanding\n",
        "### 1.1 Background\n",
        "\n",
        "XYZ Company is a leading telecommunications provider known for its innovative services and customer-centric approach. Operating in a highly competitive market, XYZ Company offers a wide range of telecom solutions, including mobile, broadband, and digital services, catering to both individual and business customers. Despite its strong market presence, XYZ Company faces challenges related to customer churn, a critical issue that impacts its revenue and growth. By leveraging data analytics and customer insights, XYZ Company aims to mitigate churn and enhance customer retention, ensuring long-term sustainability and profitability in the dynamic telecom industry.\n",
        "\n",
        "Customer churn in the telecom industry refers to the rate at which existing customers discontinue using a telecom service within a specific period. This phenomenon is critical because it directly affects a company's revenue and profitability. Customers may churn due to dissatisfaction with service quality, uncompetitive pricing, poor customer support, or more attractive offers from competitors. Understanding and managing churn is essential for telecom companies to maintain a stable customer base and financial health (O'Brien & Downie, 2024; Britto, 2024; Luck, 2023; Investopedia Team, 2024).\n",
        "\n",
        "Churn is essentially the opposite of customer retention, measuring the number of customers lost over a given timeframe. In subscription-based services like telecom, churn immediately impacts revenue since it involves the loss of recurring payments. In the telecom sector, churn is particularly significant because high churn rates can lead to financial losses and hinder growth. Acquiring new customers is often more expensive than retaining existing ones, making it vital for companies to focus on reducing churn by improving service quality, offering competitive pricing, and enhancing customer engagement (O'Brien & Downie, 2024; Luck, 2023; Britto, 2024; Investopedia Team, 2024).\n",
        "\n",
        "### 1.2 Gap Analysis\n",
        "The gap analysis for the telecom customer churn prediction project identifies key areas for improvement. Currently, the company lacks a predictive model to identify customers at risk of churning, which limits its ability to implement effective retention strategies. There are significant gaps in data quality, stakeholder engagement, and model interpretability, which can hinder informed decision-making. Additionally, establishing a continuous improvement process is essential to adapt to changing customer behaviors, along with integrating predictive insights into operational workflows. By addressing these gaps, the company can reduce churn rates, enhance customer retention, and improve overall business performance.\n",
        "\n",
        "### 1.3 Problem Statements\n",
        "- Which customers are likely to churn?\n",
        "- What factors influence customer churn the most?\n",
        "- How can the company reduce churn and improve customer retention?\n",
        "\n",
        "### 1.4 Goals\n",
        "- Develop a predictive model to classify customers as churn or no churn.\n",
        "    + In the telecom sector, predicting which customers are likely to churn involves analyzing customer data to identify patterns and behaviors that indicate a high risk of leaving. This can be achieved by using machine learning models trained on historical data. Classification models are commonly used for this purpose (Kumari et al., 2025; IQ Team, 2025). By identifying at-risk customers, companies can implement targeted retention strategies to reduce churn.\n",
        "- Identify key features affecting churn.\n",
        "    + Understanding the factors that drive customer churn is essential for developing effective retention strategies. Feature importance analysis from machine learning models helps identify which attributes have the strongest impact on churn likelihood. For instance, high monthly charges or poor customer service experiences are often significant predictors of churn (Kumari et al., 2025; Bhatnagar & Srivastava, 2025). By focusing on these key factors, businesses can optimize their services and improve customer satisfaction.\n",
        "- Provide actionable insights for business to reduce churn.\n",
        "    + To reduce churn and enhance retention, telecom companies can leverage insights from predictive models to implement targeted strategies. These may include offering personalized plans, discounts, or improved customer support to at-risk customers. Additionally, addressing common issues highlighted by churn predictors—such as high costs or poor service quality—can reduce overall churn rates. By combining predictive insights with business actions, companies can maximize customer lifetime value and reduce acquisition costs, which are typically higher than retention costs (BlastChar, 2018; Bhatnagar & Srivastava, 2025).\n",
        "\n",
        "### 1.5 Analytical Approach\n",
        "According to Hermawan et al. (2024), it is common for businesses like TELCO Company to utilize a rule-based strategy. However, this technique frequently fails to identify nuanced patterns in client behavior. With the advancement of technology, machine learning algorithms are becoming increasingly important for the analysis of enormous volumes of data and the discovery of hidden patterns. This enables businesses to more correctly identify clients who are at risk of leaving their services. Therefore, there will be 2 approaches benchmarked in this case, which are rule-based strategy and machine learing-based strategy.\n",
        "\n",
        "### 1.6 Metric Evaluation\n",
        "- Business Metric 1: Customer Acquistion Rate (CAC)\n",
        "    + Customer Acquisition Cost (CAC) is a crucial business metric that measures the total cost of acquiring a new customer, including sales and marketing expenses. It helps businesses evaluate efficiency and profitability, identify cost-effective channels, and make strategic decisions to improve marketing return on investment and customer retention (Corporate Finance Institute [CFI], n.d.). Yoga (2024) assumes that the CAC can be $200 per customer.\n",
        "\n",
        "- Business Metric 2: Customer Retention Rate (CRC)\n",
        "    + Customer Retention Cost (CRC) is the cost a company incurs to retain and retain customers, including service salaries, loyalty programs, training, and account management (Singerland,2023). Yoga (2024) assumes that the CRC can be $50 per customer.\n",
        "\n",
        "- Machine Learning Evaluation Metric: Recall\n",
        "    + Recall is a vital metric in telecom churn prediction because it directly reflects the model’s effectiveness in capturing the customers most at risk of leaving, enabling targeted retention efforts that protect revenue and support business growth.\n",
        "\n",
        "### 1.7 Success Criteria\n",
        "- Achieve high classification performance on recall (>=80).\n",
        "- Provide model that can reduce Customer Acquisition Cost and Retention Cost.\n",
        "- Provide interpretable insights for business decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2. Data Understanding\n",
        "### 2.1 Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c94b2a2",
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Disable LOKY debug logs\n",
        "os.environ[\"LOKY_DEBUG\"] = \"0\"\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "\n",
        "# Model explainability\n",
        "import lime.lime_tabular\n",
        "\n",
        "# Imbalanced-learn (sampling)\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.over_sampling import (\n",
        "    ADASYN,\n",
        "    BorderlineSMOTE,\n",
        "    KMeansSMOTE,\n",
        "    RandomOverSampler,\n",
        "    SMOTE,\n",
        "    SMOTENC,\n",
        "    SMOTEN,\n",
        "    SVMSMOTE,\n",
        ")\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    BaggingClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    StackingClassifier,\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    # accuracy_score,\n",
        "    # brier_score_loss,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    # f1_score,\n",
        "    precision_recall_curve,\n",
        "    # precision_score,\n",
        "    recall_score,\n",
        "    # roc_auc_score,\n",
        "    # roc_curve,\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    StratifiedKFold,\n",
        "    RandomizedSearchCV,\n",
        "    cross_val_score,\n",
        "    learning_curve,\n",
        "    train_test_split,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    FunctionTransformer,\n",
        "    MinMaxScaler,\n",
        "    OneHotEncoder,\n",
        "    OrdinalEncoder,\n",
        "    RobustScaler,\n",
        ")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Gradient boosting frameworks\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Joblib for model persistence\n",
        "import joblib\n",
        "\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set pandas display options for better visibility\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "# Set Seaborn theme for better aesthetics\n",
        "sns.set_theme()\n",
        "\n",
        "\n",
        "data_path = Path(r\"c:\\Users\\User\\Downloads\\final-project-alpha-team\\data\\WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
        "\n",
        "if data_path.exists():\n",
        "    real_df = pd.read_csv(data_path)\n",
        "    df = real_df.copy()\n",
        "    print(df.info())\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(f\"File not found: {data_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d430b2f3",
      "metadata": {},
      "source": [
        "The provided summary of the DataFrame indicates that it contains a total of 4,930 entries, which represent individual customers or accounts. The DataFrame is structured with 11 columns, each capturing specific attributes related to the customers' service usage and characteristics.\n",
        "\n",
        "Among the columns, the Dependents field identifies whether customers have dependents, while tenure reflects the duration, in months, that customers have maintained their service. Several columns, such as OnlineSecurity, OnlineBackup, DeviceProtection, and TechSupport, denote the availability of various service options, illustrating the range of features customers can choose. The InternetService column specifies the type of internet service provided, which may influence customer satisfaction and retention.\n",
        "\n",
        "Additionally, the Contract column indicates the nature of the customer's agreement with the service provider, and PaperlessBilling shows whether customers have opted for electronic billing. The MonthlyCharges column presents the cost incurred by each customer monthly, providing insight into the pricing structure. Finally, the Churn column indicates whether a customer has discontinued their service, which is crucial for understanding customer retention and satisfaction. In details, the column breakdowns as per below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf5ecb48",
      "metadata": {},
      "source": [
        "<table>\n",
        "    <tr>\n",
        "        <th>Column Name</th>\n",
        "        <th>Importance</th>\n",
        "        <th>Impact to Business</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Dependents</td>\n",
        "        <td>Moderate</td>\n",
        "        <td>Understanding customer demographics can aid in targeted marketing strategies.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Tenure</td>\n",
        "        <td>High</td>\n",
        "        <td>Longer tenure often indicates customer loyalty, impacting retention strategies.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>OnlineSecurity</td>\n",
        "        <td>High</td>\n",
        "        <td>Customers with online security are likely to feel safer, reducing churn.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>OnlineBackup</td>\n",
        "        <td>Moderate</td>\n",
        "        <td>Offering online backup can enhance customer satisfaction and retention.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>InternetService</td>\n",
        "        <td>High</td>\n",
        "        <td>Understanding service subscriptions helps in optimizing service offerings.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>DeviceProtection</td>\n",
        "        <td>Moderate</td>\n",
        "        <td>Device protection can be a key selling point for tech-savvy customers.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>TechSupport</td>\n",
        "        <td>High</td>\n",
        "        <td>Good tech support can significantly reduce churn and improve customer satisfaction.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Contract</td>\n",
        "        <td>High</td>\n",
        "        <td>Contract types influence customer retention and revenue predictability.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>PaperlessBilling</td>\n",
        "        <td>Moderate</td>\n",
        "        <td>Encouraging paperless billing can reduce costs and appeal to environmentally conscious customers.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>MonthlyCharges</td>\n",
        "        <td>High</td>\n",
        "        <td>Understanding pricing impacts customer acquisition and retention strategies.</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>Churn</td>\n",
        "        <td>Critical</td>\n",
        "        <td>Churn rate is a key performance indicator for business health and customer satisfaction. This column will be used as target.</td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e73c8b3",
      "metadata": {},
      "source": [
        "### 2.2 Missing Values Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4c2f08",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check missing values count per column\n",
        "missing_counts = df.isnull().sum()\n",
        "print(missing_counts)\n",
        "\n",
        "# Visualize missing data pattern\n",
        "msno.matrix(df)\n",
        "plt.title('Missing Data Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d359e9",
      "metadata": {},
      "source": [
        "The summary indicates that there are no missing values across any of the columns in the DataFrame, which consists of 4,930 entries. Each attribute, including Dependents, tenure, OnlineSecurity, OnlineBackup, InternetService, DeviceProtection, TechSupport, Contract, PaperlessBilling, MonthlyCharges, and Churn, has a complete dataset with zero missing entries. This completeness is crucial for conducting accurate analyses and drawing reliable conclusions.\n",
        "\n",
        "The accompanying visual representation, a missing data matrix, further emphasizes this finding. Each column is displayed without any gaps, confirming that all data points are accounted for. This absence of missing values enhances the integrity of the dataset, allowing for more robust statistical evaluations and insights into customer behavior and service usage. Overall, the DataFrame is well-prepared for further analysis, ensuring that any insights derived will be based on a comprehensive and complete dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c95e8abc",
      "metadata": {},
      "source": [
        "### 2.3 Duplicated Values Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a75554d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count duplicated rows\n",
        "num_duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicated rows: {num_duplicates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36d33d19",
      "metadata": {},
      "source": [
        "The presence of 77 duplicated rows in this dataset indicates that these rows are exact copies of others within the same data. Such duplicates can arise from data entry errors, system issues, or legitimate repeated events depending on the context. However, removing duplicates is not always the best course of action.\n",
        "\n",
        "One reason to retain duplicates is that they may represent valid repeated observations or transactions, which are important for accurate analysis. For example, in transactional data or repeated measurements, duplicates reflect real-world occurrences rather than errors. Removing them could distort data distributions and lead to misleading results. Additionally, duplicates can help validate data consistency and quality during data processing (Oracle Community, 2024).\n",
        "\n",
        "Moreover, certain analytical methods and visualizations rely on the frequency of data points, where duplicates provide meaningful information rather than noise. Understanding the origin and significance of duplicates is essential before deciding to remove them, as their removal may compromise data integrity and analytical validity (Imhoff, Galemmo, & Geiger, 2003).\n",
        "\n",
        "In summary, while duplicates often indicate data quality issues, they can also carry important information depending on the dataset and context. Careful assessment ensures that valuable data is preserved, supporting reliable and valid analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddd758a",
      "metadata": {},
      "source": [
        "### 2.4 Dataset Restructuring for Better EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "330de03a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert object columns to category dtype\n",
        "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "df[object_cols] = df[object_cols].astype('category')\n",
        "\n",
        "# Confirm changes\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dfcd25",
      "metadata": {},
      "source": [
        "The DataFrame is restructured for better Exploratory Data Analysis (EDA) because the initial data types are not relevant. This restructuring is performed to enhance the efficiency and clarity of the data, which is crucial for effective analysis. The object columns are converted to categorical data type, a process that is often necessary to ensure that the data is properly formatted for statistical and machine learning tasks (McKinney, 2017; VanderPlas, 2016).\n",
        "\n",
        "The conversion of object columns to categorical data type is achieved by using the astype('category') method in Pandas. This approach not only improves memory efficiency but also ensures that the data is treated appropriately during analysis. The changes are confirmed by checking the updated data types of the DataFrame using the info() method, which provides a summary of the DataFrame's structure and memory usage (McKinney, 2017; VanderPlas, 2016).\n",
        "\n",
        "The restructuring process is facilitated by identifying the object columns and applying the necessary conversion. This step is essential in maintaining data integrity and ensuring that the analysis is conducted on the correct data types. The use of categorical data types can also help in reducing errors that might arise from treating categorical variables as strings (Wickham & Grolemund, 2017; McKinney, 2017)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67c6e0ae",
      "metadata": {},
      "source": [
        "### 2.5 Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93de012",
      "metadata": {},
      "source": [
        "#### 2.5.1 Statistics Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18b61e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8d9068",
      "metadata": {},
      "source": [
        "The dataset includes information on customer tenure and monthly charges for 4,930 customers. The average tenure is about 32.4 months, with some customers just starting and others staying up to 72 months. Most customers have been with the service for less than 55 months, with 25% staying 9 months or less. Monthly charges vary widely, averaging around 64.88 units. Charges range from 18.8 to 118.65 units, with half of the customers paying less than 70.35 units. Overall, the data shows a diverse range of customer lengths and monthly payments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321a3fc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe(include=['object','category']).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cabeba",
      "metadata": {},
      "source": [
        "For the \"Dependents\" variable, there are two categories, with the majority (3,446 customers) reporting no dependents. The \"OnlineSecurity\" and \"OnlineBackup\" features each have three categories, with \"No\" being the most common response, reported by 2,445 and 2,172 customers respectively. The \"InternetService\" variable also has three categories, with \"Fiber optic\" being the most frequent choice at 2,172 customers.\n",
        "\n",
        "Regarding \"DeviceProtection\" and \"TechSupport,\" both have three categories, and \"No\" is the most common response, with 2,186 and 2,467 customers respectively. The \"Contract\" variable has three categories, with \"Month-to-month\" contracts being the most frequent at 2,721 customers. For \"PaperlessBilling,\" there are two categories, with the majority (2,957 customers) opting for paperless billing. Finally, the \"Churn\" variable, which likely indicates whether a customer has left the service, has two categories, with most customers (3,614) not having churned."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac9f916",
      "metadata": {},
      "source": [
        "#### 2.5.2 Proportion Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7024d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "churn_proportion = df['Churn'].value_counts(normalize=True) * 100\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(churn_proportion, labels=churn_proportion.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Churn Proportion')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures pie chart is circular\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3476b2a5",
      "metadata": {},
      "source": [
        "The pie chart displays the proportion of customers who churn versus those who do not. It shows that approximately 26.7% of customers have churned, while the majority, 73.3%, have remained with the service. This indicates that about one in four customers choose to leave, highlighting a notable churn rate. In terms of dataset balance, this distribution is considered imbalanced because the classes are not evenly represented—about 73% of customers stay while only 27% churn. Commonly, a dataset is regarded as imbalanced if one class exceeds 70% of the total observations (He & Garcia, 2009). Such imbalance can affect the performance of predictive models, as many algorithms may be biased toward the majority class. Special techniques such as resampling, class weighting, or using algorithms designed to handle imbalance may be needed to build effective churn prediction models (He & Garcia, 2009)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "444f7b79",
      "metadata": {},
      "source": [
        "#### 2.5.3 Boxplot and Histogram Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d0c4ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numeric columns\n",
        "numerics = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# Create subplots: 2 rows, 2 columns\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# 1st subplot: Boxplot of 'tenure'\n",
        "sns.boxplot(x=df['tenure'], orient='h', ax=axes[0])\n",
        "axes[0].set_title('Boxplot of Tenure')\n",
        "\n",
        "# 2nd subplot: Boxplot of 'MonthlyCharges'\n",
        "sns.boxplot(x=df['MonthlyCharges'], orient='h', ax=axes[1])\n",
        "axes[1].set_title('Boxplot of Monthly Charges')\n",
        "\n",
        "# 3rd subplot: KDE plot for 'tenure' by 'Churn'\n",
        "sns.kdeplot(data=df, x='tenure', hue='Churn', fill=True, ax=axes[2])\n",
        "axes[2].set_title('Density Plot of Tenure by Churn')\n",
        "axes[2].legend(title='Churn')\n",
        "\n",
        "# 4th subplot: KDE plot for 'MonthlyCharges' by 'Churn'\n",
        "sns.kdeplot(data=df, x='MonthlyCharges', hue='Churn', fill=True, ax=axes[3])\n",
        "axes[3].set_title('Density Plot of Monthly Charges by Churn')\n",
        "axes[3].legend(title='Churn')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cde4d4",
      "metadata": {},
      "source": [
        "The tenure boxplot shows that customer tenure ranges widely, from 0 to about 72 months. The median tenure is around 29 months, meaning half of the customers stay less than two and a half years, and the other half stay longer. Most customers fall between 9 and 55 months, with no extreme outliers. The data is fairly balanced between short-term and long-term customers.\n",
        "\n",
        "The monthly charges boxplot indicates that charges vary from about $18 to $118. The median charge is about $70, with most customers paying between $37 and $91. Like tenure, the distribution is fairly even with no unusual values. This suggests that while monthly charges differ, most customers pay within a moderate range. Overall, these plots show clear and consistent patterns in customer tenure and billing.\n",
        "\n",
        "The density plots reveal important differences between customers who churn and those who do not. For tenure, customers who churn tend to have much shorter tenures, with a peak near zero months, indicating they leave early. In contrast, customers who do not churn show a bimodal distribution with peaks around 1–2 months and again near 70 months, reflecting both new and long-term loyal customers. For monthly charges, customers who churn generally have lower charges, mostly below $40, while those who stay tend to have higher charges spread across a wider range, with peaks near $20 and between $60 and $100.\n",
        "\n",
        "Since the data is not normally distributed and does not contain outliers, careful consideration should be given to the choice of scaling method to ensure effective model performance. Both Min-Max Scaling and Robust Scaling are recommended to be benchmarked. With Min-Max Scaling, the data is rescaled to a fixed range, typically between 0 and 1, allowing the original distribution shape to be preserved and making it suitable for algorithms requiring normalized inputs (Jain, 2010). In contrast, Robust Scaling is applied using the median and interquartile range, which makes it less sensitive to any potential outliers or skewness, even if minimal (Liu et al., 2020). By comparing both methods, it can be determined which scaling approach better suits the specific dataset and modeling goals, balancing the preservation of data distribution with robustness to subtle irregularities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1a1608",
      "metadata": {},
      "source": [
        "#### 2.5.4 Unique Values Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b090d694",
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_cols = df.select_dtypes(include=['category']).columns\n",
        "\n",
        "# Dictionary of unique counts per categorical column\n",
        "unique_counts = {col: df[col].nunique() for col in cat_cols}\n",
        "\n",
        "# Dictionary of unique values lists per categorical column\n",
        "unique_values_list = {col: df[col].unique().tolist() for col in cat_cols}\n",
        "\n",
        "unique_summary = pd.DataFrame({'unique_counts': unique_counts, 'unique_values': unique_values_list})\n",
        "unique_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c456282f",
      "metadata": {},
      "source": [
        "The dataset contains several categorical variables that are primarily nominal in nature. Variables such as \"Dependents,\" \"OnlineSecurity,\" \"OnlineBackup,\" \"DeviceProtection,\" \"TechSupport,\" \"InternetService,\" \"PaperlessBilling,\" and the target variable \"Churn\" are classified as nominal. These variables consist of categories that represent distinct groups without any inherent order or ranking. For example, the categories \"Yes,\" \"No,\" and \"No internet service\" in service-related features serve as labels rather than ordered values. Nominal variables require encoding methods that do not impose ordinal relationships, such as one-hot encoding (Liu et al., 2020).\n",
        "\n",
        "The only variable identified as ordinal is \"Contract,\" which includes categories like \"Month-to-month,\" \"One year,\" and \"Two year.\" These categories have a natural order based on contract duration, which implies a ranking that should be preserved during encoding. Ordinal variables can be encoded using techniques such as label encoding or ordinal encoding to maintain the meaningful sequence of categories (Liu et al., 2020).\n",
        "\n",
        "For encoding, it is recommended that nominal variables be transformed using one-hot encoding to avoid introducing artificial order, while the ordinal variable should be encoded with methods that preserve the inherent order of its categories. This approach ensures that the model accurately interprets the categorical data without biasing relationships among categories. Proper encoding is essential for effective feature representation and improved model performance in telecom churn prediction (He & Garcia, 2009; Kumari et al., 2025)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0c54f2",
      "metadata": {},
      "source": [
        "#### 2.5.5 Feature Distribution Analysis (Categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34514367",
      "metadata": {},
      "outputs": [],
      "source": [
        "# categorical_cols = df.select_dtypes(include='category').columns.drop('Churn')\n",
        "\n",
        "# num_plots = len(categorical_cols)\n",
        "# cols = 2\n",
        "# rows = (num_plots + 1) // cols\n",
        "\n",
        "# fig, axes = plt.subplots(rows, cols, figsize=(cols * 10, rows * 6))\n",
        "# axes = axes.flatten()\n",
        "\n",
        "# for i, col in enumerate(categorical_cols):\n",
        "#     ax = axes[i]\n",
        "    \n",
        "#     # Compute counts per category and churn status with observed=True\n",
        "#     counts = df.groupby([col, 'Churn'], observed=True).size().unstack(fill_value=0)\n",
        "    \n",
        "#     # Convert counts to percentages per category level (row-wise)\n",
        "#     percentages = counts.div(counts.sum(axis=1), axis=0) * 100\n",
        "    \n",
        "#     # Plot horizontal stacked bar chart on the subplot axis\n",
        "#     percentages.plot(kind='barh', stacked=True, ax=ax, legend=False)\n",
        "    \n",
        "#     ax.set_title(f'{col}')\n",
        "#     ax.set_xlabel('Percentage')\n",
        "#     ax.set_ylabel('')\n",
        "    \n",
        "#     # Annotate each segment with percentage and count\n",
        "#     for j, (index, row) in enumerate(percentages.iterrows()):\n",
        "#         cum_width = 0\n",
        "#         for churn_status in percentages.columns:\n",
        "#             pct = row[churn_status]\n",
        "#             cnt = counts.loc[index, churn_status]\n",
        "#             if pct > 0:\n",
        "#                 label = f'{pct:.1f}%\\n({cnt})'\n",
        "#                 ax.text(cum_width + pct / 2, j, label, ha='center', va='center', fontsize=8,\n",
        "#                         color='white' if pct > 15 else 'black')\n",
        "#                 cum_width += pct\n",
        "\n",
        "# # Remove any unused subplots\n",
        "# for k in range(i + 1, len(axes)):\n",
        "#     fig.delaxes(axes[k])\n",
        "\n",
        "# # Create one legend for all plots\n",
        "# handles, labels = ax.get_legend_handles_labels()\n",
        "# fig.legend(handles, labels, title='Churn', loc='upper right')\n",
        "\n",
        "# plt.tight_layout(rect=[0, 0, 0.9, 1])  # Leave space on right for legend\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35ffb96",
      "metadata": {},
      "source": [
        "The visualizations display the distribution of customer churn across various categorical features in the telecom dataset. For the variable Dependents, customers without dependents show a notably higher churn rate (31.7%) compared to those with dependents (15.2%). This suggests that customers with dependents tend to stay longer with the service.\n",
        "\n",
        "In terms of OnlineSecurity, customers who do not have this service exhibit a higher churn rate (41.8%) compared to those who have it (14.8%) or those without internet service (7.8%). A similar pattern is observed for OnlineBackup, where customers lacking the service show a churn rate of 40.7%, much higher than customers with the service (20.5%) or no internet service (7.8%). This trend indicates that the presence of security and backup services may reduce the likelihood of churn.\n",
        "\n",
        "The InternetService type also influences churn rates. Customers using Fiber optic service have the highest churn rate at 42.2%, while those with DSL service churn at 18.6%, and customers without internet service churn the least at 7.8%. This suggests that fiber optic customers may be more prone to leaving, possibly due to service or pricing factors.\n",
        "\n",
        "For DeviceProtection, customers without the service show a churn rate of 38.9%, which is considerably higher than those with the service (22.6%) or no internet service (7.8%). Similarly, TechSupport follows this pattern, with a churn rate of 41.3% for customers without support, compared to 15.2% for those with it and 7.8% for those without internet.\n",
        "\n",
        "The Contract type exhibits a strong relationship with churn. Customers on a month-to-month contract have the highest churn rate (43.3%), while those with one-year and two-year contracts have much lower churn rates of 10.1% and 2.8%, respectively. This confirms that longer contract durations are associated with better customer retention.\n",
        "\n",
        "Finally, PaperlessBilling shows that customers using paperless billing churn more (33.5%) than those who do not (16.5%). This may reflect behavioral or demographic differences that warrant further investigation.\n",
        "\n",
        "Overall, these patterns highlight that service features, contract type, and billing preferences are important factors related to customer churn. Such insights can guide targeted retention strategies and inform feature selection for predictive modeling (Kumari et al., 2025; He & Garcia, 2009)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90978186",
      "metadata": {},
      "source": [
        "#### 2.5.6 Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7095fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Compute Spearman correlation matrix for numeric columns\n",
        "# corr = df.corr(numeric_only=True, method='spearman')\n",
        "\n",
        "# # Create a mask for the upper triangle\n",
        "# mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# # Create subplots\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# # 1st subplot: KDE plot for 'tenure' by 'Churn'\n",
        "# sns.heatmap(corr, mask=mask, annot=True, cmap='YlGnBu', vmin=-1, vmax=1, square=True, linewidths=0.5, ax=axes[0])\n",
        "# axes[0].set_title('Spearman Correlation Heatmap (Lower Triangle)')\n",
        "\n",
        "# # 2nd subplot: Scatter plot for 'MonthlyCharges' vs 'tenure' colored by 'Churn'\n",
        "# sns.scatterplot(data=df, x='tenure', y='MonthlyCharges', hue='Churn', ax=axes[1], alpha=0.6)\n",
        "\n",
        "# # Add regression line to the scatter plot\n",
        "# sns.regplot(data=df, x='tenure', y='MonthlyCharges', ax=axes[1], scatter=False, color='black')\n",
        "\n",
        "# axes[1].set_title('Monthly Charges vs Tenure')\n",
        "\n",
        "# # Show the legend\n",
        "# for ax in axes:\n",
        "#     ax.legend(title='Churn')\n",
        "\n",
        "# # Adjust layout\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9999abf",
      "metadata": {},
      "source": [
        "The Spearman rank correlation coefficient is a non-parametric measure that assesses the strength and direction of a monotonic relationship between two variables without assuming linearity or normal distribution of data (Mukaka, 2012). It quantifies how well the relationship between two variables can be described using a monotonic function, where values closer to +1 or -1 indicate stronger positive or negative monotonic relationships, respectively (Mukaka, 2012; Schober et al., 2018).\n",
        "\n",
        "Interpretation guidelines often categorize correlation coefficients as weak, moderate, or strong, with coefficients around 0.1 to 0.3 considered weak, 0.3 to 0.5 moderate, and above 0.5 strong (Schober et al., 2018). Therefore, a coefficient of 0.27, as observed between Monthly Charges and Tenure, suggests a weak to moderate positive monotonic association, meaning that as tenure increases, monthly charges tend to increase as well, though the relationship is not very strong.\n",
        "\n",
        "The Spearman correlation heatmap and scatter plot provide insights into the relationship between Monthly Charges and Tenure in the telecom dataset. The heatmap shows a positive correlation coefficient of 0.27 between these two variables, indicating a weak to moderate positive monotonic relationship. This suggests that as customers’ tenure increases, their monthly charges tend to increase slightly.\n",
        "\n",
        "The scatter plot further illustrates this relationship by plotting individual customer data points, with tenure on the x-axis and monthly charges on the y-axis. A positive trend line confirms the upward trend, showing that customers with longer tenure generally incur higher monthly charges. The plot also differentiates customers based on churn status, with churned customers (in orange) scattered throughout but more concentrated at lower tenure values and higher monthly charges. This pattern implies that customers who pay higher monthly charges but have shorter tenure are more likely to churn.\n",
        "\n",
        "Overall, these findings suggest that tenure and monthly charges are related, and their interaction is important for understanding customer behavior. The weak correlation indicates that while monthly charges tend to increase with tenure, other factors also influence churn. These insights can guide feature engineering and model development for churn prediction in telecom (He & Garcia, 2009; Kumari et al., 2025)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee8d09b9",
      "metadata": {},
      "source": [
        "## Section 3. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8587057",
      "metadata": {},
      "source": [
        "### 3.1 Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa4f78b",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['TotalCharges'] = df['tenure'] * df['MonthlyCharges']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb6e1a7a",
      "metadata": {},
      "source": [
        "A new feature named \"TotalCharges\" is created by multiplying the \"tenure\" and \"MonthlyCharges\" columns. This new variable represents the total amount charged to a customer over their entire tenure with the company. The calculation is performed to capture the cumulative revenue generated from each customer. By including this feature, additional insights into customer value and behavior can be provided. The dataset is then updated to include this newly engineered feature for further analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a785b5ab",
      "metadata": {},
      "source": [
        "### 3.2 Binning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90581681",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['tenure_bin'], tenure_bins = pd.qcut(df['tenure'], q=4, labels=[1, 2, 3, 4], retbins=True)\n",
        "df['MonthlyCharges_bin'], monthly_bins = pd.qcut(df['MonthlyCharges'], q=4, labels=[1, 2, 3, 4], retbins=True)\n",
        "df['TotalCharges_bin'], total_bins = pd.qcut(df['TotalCharges'], q=4, labels=[1, 2, 3, 4], retbins=True)\n",
        "\n",
        "def create_intervals(edges):\n",
        "    intervals = []\n",
        "    # q1: less than first edge after min\n",
        "    intervals.append(f\"< {edges[1]}\")\n",
        "    # q2, q3, q4: intervals between edges\n",
        "    for i in range(1, len(edges)-1):\n",
        "        intervals.append(f\"{edges[i]} - {edges[i+1]}\")\n",
        "    return intervals\n",
        "\n",
        "bin_summary = pd.DataFrame({\n",
        "    'Bin Col Name': ['Tenure', 'MonthlyCharges', 'TotalCharges'],\n",
        "    'q1': [f\"< {tenure_bins[1]}\", f\"< {monthly_bins[1]}\", f\"< {total_bins[1]}\"],\n",
        "    'q2': [f\"{tenure_bins[1]} - {tenure_bins[2]}\", f\"{monthly_bins[1]} - {monthly_bins[2]}\", f\"{total_bins[1]} - {total_bins[2]}\"],\n",
        "    'q3': [f\"{tenure_bins[2]} - {tenure_bins[3]}\", f\"{monthly_bins[2]} - {monthly_bins[3]}\", f\"{total_bins[2]} - {total_bins[3]}\"],\n",
        "    'q4': [f\"{tenure_bins[3]} - {tenure_bins[4]}\", f\"{monthly_bins[3]} - {monthly_bins[4]}\", f\"{total_bins[3]} - {total_bins[4]}\"],\n",
        "})\n",
        "\n",
        "display(bin_summary)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "493dd315",
      "metadata": {},
      "source": [
        "The code uses the pandas function pd.qcut to segment the continuous variables 'tenure', 'MonthlyCharges', and 'TotalCharges' into four quantile-based bins. This method divides the data into bins such that each bin contains an approximately equal number of observations, which is particularly useful when domain knowledge is limited and there are no natural thresholds for categorization. Quantile binning is a data-driven discretization technique that helps in handling continuous variables by creating balanced groups based on the empirical distribution of the data, avoiding arbitrary cutoffs and improving interpretability and model robustness (Liu et al., 2002).\n",
        "\n",
        "Regarding the 'Tenure' variable, the data is divided into four intervals that represent the length of time customers have stayed with the service. The first quartile includes customers with tenure less than 9 months, indicating relatively new customers. The second quartile covers tenures between 9 and 29 months, while the third spans from 29 to 55 months. The fourth quartile represents long-term customers with tenure ranging from 55 to 72 months. This segmentation helps in analyzing customer retention patterns across different durations.\n",
        "\n",
        "For 'MonthlyCharges', the segmentation reflects the monthly billing amounts customers incur. The lowest quartile includes charges under $37.05, representing customers with minimal monthly expenses. The second quartile ranges from $37.05 to $70.35, and the third from $70.35 to $89.85. The highest quartile includes customers paying between $89.85 and $118.65 monthly. These ranges provide insight into the distribution of customer spending on a monthly basis.\n",
        "\n",
        "The 'TotalCharges' variable, representing the cumulative amount charged to customers, is divided into four ranges. The first quartile includes customers with total charges less than $407.25, likely newer or less active customers. The second quartile spans from $407.25 to $1400.40, the third from $1400.40 to approximately $3778.54, and the fourth quartile includes customers with total charges up to $8510.40. This breakdown offers a perspective on overall customer value and engagement over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "704f8d06",
      "metadata": {},
      "source": [
        "### 3.2 Target Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aceb9f0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map target variable 'Churn' from Yes/No to 1/0\n",
        "df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8034dd23",
      "metadata": {},
      "source": [
        "The target variable \"Churn\" is transformed from categorical labels \"Yes\" and \"No\" into binary numerical values 1 and 0, respectively. This mapping facilitates the use of machine learning algorithms that require numerical input for classification tasks. The conversion standardizes the target variable, allowing for efficient model training and evaluation. After the transformation, the dataset is updated to reflect the binary encoding of the churn status."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "124aff8a",
      "metadata": {},
      "source": [
        "### 3.3 Define Features and Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46dcfa72",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature (X) and target (y)\n",
        "X = df.drop(columns=['Churn'])\n",
        "y = df['Churn']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bd328ad",
      "metadata": {},
      "source": [
        "The feature set (X) is defined by removing the 'Churn' column from the dataset, which contains the input variables for a machine learning model. Meanwhile, the target variable (y) is set to the 'Churn' column of the same DataFrame, representing the outcome that the model aims to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f3eb78",
      "metadata": {},
      "source": [
        "### 3.4 Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf3fa4fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stratified split to maintain target distribution\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Calculate proportions\n",
        "full_dist = y.value_counts(normalize=True)\n",
        "train_dist = y_train.value_counts(normalize=True)\n",
        "test_dist = y_test.value_counts(normalize=True)\n",
        "\n",
        "# Create DataFrame\n",
        "df_propo = pd.DataFrame({\n",
        "    'Full Dataset': full_dist,\n",
        "    'Train Set': train_dist,\n",
        "    'Test Set': test_dist\n",
        "}).T\n",
        "\n",
        "df_propo.columns = ['Not Churn', 'Churn']  # assuming 0 = Not Churn, 1 = Churn\n",
        "\n",
        "# Plot horizontal stacked bar chart\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "# Plot 'Not Churn' bars\n",
        "bars_not_churn = ax.barh(df_propo.index, df_propo['Not Churn'], label='Not Churn')\n",
        "\n",
        "# Plot 'Churn' bars on top of 'Not Churn'\n",
        "bars_churn = ax.barh(df_propo.index, df_propo['Churn'], left=df_propo['Not Churn'], label='Churn')\n",
        "\n",
        "ax.set_xlabel('Proportion')\n",
        "ax.set_title('Proportion of Churn vs Not Churn in Dataset Splits')\n",
        "ax.legend()\n",
        "plt.xlim(0, 1)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Annotate the percentages\n",
        "for i, idx in enumerate(df_propo.index):\n",
        "    not_churn_val = df_propo.loc[idx, 'Not Churn']\n",
        "    churn_val = df_propo.loc[idx, 'Churn']\n",
        "    \n",
        "    # Annotate Not Churn\n",
        "    ax.text(not_churn_val / 2, i, f\"{not_churn_val:.1%}\", va='center', ha='center', color='white', fontsize=10, fontweight='bold')\n",
        "    \n",
        "    # Annotate Churn, only if churn_val > 0 to avoid overlapping text on zero-width bars\n",
        "    if churn_val > 0:\n",
        "        ax.text(not_churn_val + churn_val / 2, i, f\"{churn_val:.1%}\", va='center', ha='center', color='white', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ce3789",
      "metadata": {},
      "source": [
        "The train-test split procedure is a common model validation technique used to simulate how a machine learning model performs on new, unseen data. Typically, the dataset is divided into a training set, used to train the model, and a testing set, used to evaluate its performance. Common practice often involves allocating around 70% to 80% of the data for training and the remaining 20% to 30% for testing, with an 80:20 split considered a good balance between training the model sufficiently and having enough data to assess its generalization. This split helps reduce overfitting and provides a reliable estimate of model performance on future data. The stratified splitting method is recommended when dealing with imbalanced datasets to preserve the proportion of classes in both subsets, further improving model evaluation reliability (Galarnyk, 2025). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5782a9f1",
      "metadata": {},
      "source": [
        "### 3.5 Data Transformation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa1df9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical columns excluding target\n",
        "categorical_cols = [col for col in df.select_dtypes(include=['category']).columns if col not in ['Churn', 'Contract']]\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Identify binary columns (with exactly 2 unique values excluding 'Churn' and 'Contract')\n",
        "binary_cols = unique_summary[unique_summary['unique_counts'] == 2].index.tolist()\n",
        "binary_cols = [col for col in binary_cols if col not in ['Churn', 'Contract']]\n",
        "\n",
        "# Separate categorical columns into binary and non-binary\n",
        "categorical_cols_no_binary = [col for col in categorical_cols if col not in binary_cols]\n",
        "\n",
        "# Define the ordinal categories for 'Contract'\n",
        "contract_categories = [['Month-to-month', 'One year', 'Two year']]\n",
        "\n",
        "# Function to map 'No'->0 and 'Yes'->1 in binary columns\n",
        "def map_binary_yes_no(X):\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X = pd.DataFrame(X, columns=binary_cols)\n",
        "    for col in X.columns:\n",
        "        X[col] = X[col].map({'No': 0, 'Yes': 1})\n",
        "        if X[col].isnull().any():\n",
        "            raise ValueError(f\"Unexpected values found in column {col} during binary mapping.\")\n",
        "        X[col] = X[col].astype(np.int64)\n",
        "    return X.values\n",
        "\n",
        "# Function to create numeric transformer pipeline based on the scaler type\n",
        "scalers = [RobustScaler(), MinMaxScaler()]\n",
        "def create_numeric_transformer(scaler):\n",
        "    return Pipeline(steps=[\n",
        "        ('scaler', scaler)\n",
        "    ])\n",
        "\n",
        "# Function to create preprocessor with the desired scaling method\n",
        "def create_preprocessor(scaler):\n",
        "    numeric_transformer = create_numeric_transformer(scaler)\n",
        "\n",
        "    binary_transformer = Pipeline(steps=[\n",
        "        ('map_yes_no', FunctionTransformer(map_binary_yes_no))\n",
        "    ])\n",
        "\n",
        "    ordinal_transformer = Pipeline(steps=[\n",
        "        ('ordinal', OrdinalEncoder(categories=contract_categories))\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_cols),\n",
        "            ('binary', binary_transformer, binary_cols),\n",
        "            ('ordinal', ordinal_transformer, ['Contract']),\n",
        "            ('cat', categorical_transformer, categorical_cols_no_binary)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ce8af0",
      "metadata": {},
      "source": [
        "Several steps are taken to preprocess a dataset for a machine learning task, ensuring that the data is appropriately prepared for modeling. First, categorical columns are identified while excluding the target variable 'Churn' and the 'Contract' column to focus on relevant features. Numeric columns are also identified based on their data types to facilitate appropriate scaling. Additionally, binary columns, which contain exactly two unique values, are isolated, and the categorical columns are further divided into binary and non-binary categories for targeted processing.\n",
        "\n",
        "A specific mapping function is defined to convert binary responses from 'No' and 'Yes' to 0 and 1, ensuring that the model can interpret these values correctly. The code includes a function to create a numeric transformer pipeline using different scalers, allowing for flexibility in handling various data distributions. Furthermore, a preprocessor is constructed that combines various transformations for numeric, binary, ordinal, and categorical data. This method is employed to streamline the preprocessing workflow, ensuring that each type of data is handled effectively, which is crucial for improving model performance (Idris, 2024). By using pipelines and transformers, the code enhances maintainability and scalability, making it easier to adapt to different datasets or modeling requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2c36e7",
      "metadata": {},
      "source": [
        "## Section 4. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f83f422",
      "metadata": {},
      "source": [
        "### 4.1 Model Initialization and Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9facf56",
      "metadata": {},
      "source": [
        "#### 4.1.1 Choosing The Classification Models\n",
        "Classification algorithms are fundamental components of machine learning, utilized to categorize data into predefined classes or labels. These algorithms operate on the principle of supervised learning, where a model is trained using a labeled dataset. The primary objective is to learn the relationships between input features and the corresponding output labels, enabling the model to predict the class of new, unseen data. Various types of classification algorithms exist, each with distinct methodologies and applications. Common types include logistic regression, decision trees, support vector machines, k-nearest neighbors, and Naïve Bayes (Belcic, 2024; GeeksforGeeks, 2024). Each algorithm possesses unique strengths and weaknesses, making them suitable for different classification tasks and datasets.\n",
        "\n",
        "The foundation of classification algorithms lies in their ability to model the relationship between input features and output labels. For instance, logistic regression employs a linear approach to predict binary outcomes by estimating probabilities. Decision trees utilize a hierarchical structure, making decisions based on feature values to classify data points (GeeksforGeeks, 2024). Support vector machines aim to find the optimal hyperplane that separates different classes in high-dimensional space. K-nearest neighbors, on the other hand, classify instances based on the majority class of their closest neighbors in the feature space. Each algorithm's effectiveness is influenced by the nature of the data, including its dimensionality, distribution, and the presence of noise (Belcic, 2024). Understanding these foundational principles is crucial for selecting the most appropriate classification algorithm for a given problem.\n",
        "\n",
        "On the other hand, ensemble learning represents a powerful technique in machine learning that combines multiple models to enhance predictive performance. The underlying principle of ensemble methods is that a group of diverse models can collectively yield better results than any single model (Murel & Kavlakoglu, 2024). Two primary types of ensemble methods are bagging and boosting. Bagging, or bootstrap aggregating, involves training multiple models independently on different subsets of the data and averaging their predictions. This approach reduces variance and helps prevent overfitting. Random forests, which are an extension of decision trees, exemplify bagging techniques (GeeksforGeeks, 2025a). Conversely, boosting trains models sequentially, where each subsequent model focuses on correcting the errors made by its predecessor. This method reduces bias and enhances overall accuracy. Popular boosting algorithms include AdaBoost and Gradient Boosting. Ensemble methods have proven particularly effective in improving model robustness and generalization, especially in complex datasets (GeeksforGeeks, 2025a).\n",
        "\n",
        "Despite its popularity and simplicity, the Naïve Bayes classifier has limitations that impact its applicability in certain scenarios. One significant drawback is its reliance on the assumption of feature independence, particularly in the Gaussian Naïve Bayes variant. This assumption may not hold true for many real-world datasets, leading to suboptimal performance (IBM, n.d.). Additionally, the Bernoulli and Multinomial variants of Naïve Bayes are sensitive to the scaling of input features, which can adversely affect classification accuracy. These models assume that features follow specific distributions, which may not align with the actual data characteristics (GeeksforGeeks, 2024). As a result, the Naïve Bayes classifier may not always be the best choice for classification tasks, especially when the underlying assumptions about feature distribution and scaling are violated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1955d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define base models for stacking\n",
        "base_models = [\n",
        "    ('lr', LogisticRegression(random_state=42, n_jobs=-1, max_iter=1000)),\n",
        "    ('rf', RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
        "    ('svc', SVC(probability=True, random_state=42)),\n",
        "    ('xgb', XGBClassifier(random_state=42, n_jobs=-1, use_label_encoder=False, eval_metric='logloss')),\n",
        "]\n",
        "\n",
        "# Define stacking classifier with logistic regression as meta-model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=LogisticRegression(random_state=42, max_iter=1000),\n",
        "    n_jobs=-1,\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "# Define models dictionary including stacking\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, n_jobs=-1, max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "    'Support Vector Machine': SVC(probability=True, random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Bagging': BaggingClassifier(random_state=42, n_jobs=-1),\n",
        "    'XGBoost': XGBClassifier(random_state=42, n_jobs=-1, use_label_encoder=False, eval_metric='logloss'),\n",
        "    'LightGBM': lgb.LGBMClassifier(random_state=42, n_jobs=-1),\n",
        "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0, thread_count=-1),\n",
        "    'Stacking': stacking_clf\n",
        "}\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = ['recall']  # Add other metrics if needed\n",
        "\n",
        "# Cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize list to store scores\n",
        "list_scores = []\n",
        "\n",
        "# Iterate over each scaler\n",
        "for scaler in scalers:\n",
        "    for model_name, model in models.items():\n",
        "        preprocessor = create_preprocessor(scaler)  # Create preprocessor with the current scaler\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', model)\n",
        "        ])\n",
        "        for score in scoring:\n",
        "            scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=score, n_jobs=-1)\n",
        "            list_scores.append({\n",
        "                'Model': model_name,\n",
        "                'Metric': score,\n",
        "                'Mean Score': np.mean(scores),\n",
        "                'Std Dev': np.std(scores),\n",
        "                'Scaler': 'Robust' if isinstance(scaler, RobustScaler) else 'MinMax'\n",
        "            })\n",
        "\n",
        "# Create DataFrame from scores\n",
        "results_df = pd.DataFrame(list_scores)\n",
        "results_df = results_df.sort_values(by=['Mean Score', 'Std Dev'], ascending=[False, True])\n",
        "display(results_df)\n",
        "\n",
        "# Find the best score for each metric\n",
        "best_scores = results_df.loc[results_df.groupby(['Metric'])['Mean Score'].idxmax()]\n",
        "\n",
        "# Extract the best model and scaler overall\n",
        "overall_best = best_scores.loc[best_scores['Mean Score'].idxmax()]\n",
        "best_model = overall_best['Model']\n",
        "best_scaler = overall_best['Scaler']\n",
        "\n",
        "# Display the best scalers for each metric\n",
        "print(\"Best Scalers for Each Metric:\")\n",
        "display(best_scores[['Metric', 'Model', 'Scaler', 'Mean Score', 'Std Dev']])\n",
        "\n",
        "print(f\"\\nOverall Best Model: {best_model} with {best_scaler} Scaler\")\n",
        "\n",
        "# Define the best scaler based on the identified best scaler\n",
        "if best_scaler == 'Robust':\n",
        "    best_scaler_instance = RobustScaler()\n",
        "else:\n",
        "    best_scaler_instance = MinMaxScaler()\n",
        "\n",
        "# Update the preprocessor variable with the best scaler\n",
        "preprocessor = create_preprocessor(best_scaler_instance)\n",
        "\n",
        "# Use the updated preprocessor in final model pipeline\n",
        "best_base_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', models[best_model])  # Use the best model from the results\n",
        "])\n",
        "\n",
        "# Display the final best pipeline\n",
        "display(best_base_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db831f41",
      "metadata": {},
      "source": [
        "#### 4.1.2 Picking The Best Benchmarking Models\n",
        "\n",
        "The provided code outlines a comprehensive machine learning pipeline that evaluates various classifiers to determine the best model and preprocessing strategy for a dataset. The pipeline includes a diverse set of base models, such as Logistic Regression, Random Forest Classifier, Support Vector Classifier (SVC), and XGBoost Classifier, which are used in a stacking classifier. The stacking classifier combines the predictions of these base models using a logistic regression model as the final estimator. This ensemble approach aims to enhance predictive performance by leveraging the strengths of multiple models.\n",
        "\n",
        "To evaluate the models, the pipeline employs recall as the primary scoring metric, which measures the ability of the classifier to accurately identify positive samples. Utilizing a Stratified K-Fold cross-validation strategy enhances model evaluation by ensuring that each fold accurately reflects the overall class distribution of the dataset, thereby providing more reliable and unbiased estimates of model performance, particularly in classification tasks with imbalanced classes (Mahesh et al., 2023). The results of the cross-validation are stored in a list, which is then converted into a DataFrame for easier analysis. The scores are sorted by mean score and standard deviation to identify the best-performing models and scalers.\n",
        "\n",
        "Ultimately, the pipeline identifies the best model for recall and the optimal scaler, which can be either a RobustScaler or a MinMaxScaler. The selected model, AdaBoost, achieves the highest mean recall score of approximately 0.5423 when combined with the Robust Scaler. The final output includes a well-structured pipeline that integrates the best preprocessing steps tailored to the dataset, ensuring that the model is effectively prepared for making predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798f756f",
      "metadata": {},
      "source": [
        "#### 4.1.3 AdaBoost In Details\n",
        "\n",
        "An AdaBoost classifier is used to improve the performance of simple classifiers by combining them into a stronger model. Initially, a basic classifier is trained on the original data. Then, subsequent classifiers are trained by paying more attention to the samples that were misclassified by previous models. This process helps the overall model focus on difficult cases and improve accuracy (scikit-learn developers, 2025).\n",
        "\n",
        "The base classifier used in AdaBoost is usually a simple decision tree with limited depth, but it can be replaced by any classifier that supports sample weighting. The number of classifiers to be combined and the influence of each classifier are controlled by parameters called nestimators and learningrate. These parameters work together to balance the model’s complexity and learning speed. The model stops adding classifiers early if a perfect fit is found (scikit-learn developers, 2025).\n",
        "\n",
        "After training, the AdaBoost model can predict classes, estimate class probabilities, and provide scores to evaluate its performance. It also offers information about the importance of each feature used in the classification. The model’s predictions are made by combining the weighted outputs of all the classifiers in the ensemble. This approach has been widely used because it is effective and relatively easy to understand (scikit-learn developers, 2025)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390af2c8",
      "metadata": {},
      "source": [
        "### 4.2 Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5efa3f6",
      "metadata": {},
      "source": [
        "#### 4.2.1 Choosing The Oversampling Techniques\n",
        "\n",
        "Several over-sampling methods are provided in the imblearn.over_sampling module to address class imbalance in datasets. Random over-sampling is performed by the RandomOverSampler class, which duplicates minority class examples randomly. Synthetic sample generation techniques are also employed, with SMOTE being a primary method that creates new samples by interpolating between existing minority class instances. Variants of SMOTE have been developed to handle different data types and scenarios. For example, SMOTENC is applied when datasets contain both categorical and continuous features, while SMOTEN is specialized for nominal features only. Adaptive synthetic sampling is implemented through the ADASYN algorithm, which focuses on generating samples for minority instances that are more difficult to learn. BorderlineSMOTE enhances the SMOTE approach by generating synthetic samples near the class boundary to improve classification performance. Additionally, clustering techniques are incorporated in KMeansSMOTE, where KMeans clustering is applied before synthetic sample generation within each cluster. SVMSMOTE uses support vector machines to identify borderline minority instances and generate synthetic samples accordingly. These methods collectively aim to improve model performance by balancing class distributions more effectively (imbalanced-learn developers, 2024a).\n",
        "\n",
        "Methods that combine over-sampling and under-sampling are provided in the imblearn.combine module to improve the handling of imbalanced datasets. Two main techniques are employed in this module. The first, SMOTEENN, performs over-sampling using SMOTE to generate synthetic minority samples, followed by cleaning with Edited Nearest Neighbours (ENN) to remove noisy or ambiguous samples. This combination aims to enhance data quality after balancing. The second technique, SMOTETomek, also applies SMOTE for over-sampling and subsequently uses Tomek links for cleaning. Tomek links identify borderline samples that are likely to be noise or overlapping between classes, and their removal helps to refine the dataset. These combined methods are designed to leverage the strengths of both over-sampling and under-sampling to create more balanced and cleaner datasets, which can improve the performance of machine learning models (imbalanced-learn developers, 2024b)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1faa047",
      "metadata": {},
      "source": [
        "#### 4.2.2 AdaBoost Parameters\n",
        "\n",
        "Several parameters can be adjusted in the AdaBoostClassifier to customize its behavior. The base estimator, which is the simple model used for boosting, can be chosen, with a shallow decision tree as the default. The number of weak learners to be combined is controlled by the n_estimators parameter, setting how many models are trained in sequence. The learning_rate determines how much each learner influences the final result, balancing learning speed and model complexity (scikit-learn developers, 2025).\n",
        "\n",
        "The algorithm parameter has been deprecated and only the ‘SAMME’ method is now supported. To ensure consistent results across runs, the random_state parameter can be set to fix the randomness during training. These options allow the AdaBoostClassifier to be tailored to different datasets and needs, making it flexible and effective (scikit-learn developers, 2025)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b96ddb",
      "metadata": {},
      "source": [
        "#### 4.2.3 Choosing RandomizedSearchCv as Hyperparameter Tuning Techniques\n",
        "\n",
        "Hyperparameter tuning is a technique in machine learning that involves selecting the best set of hyperparameters to optimize a model’s performance. Hyperparameters are configuration variables set before training that influence how the model learns, such as learning rate or number of neurons in a neural network. Unlike model parameters, which are learned from the training data, hyperparameters control the training process itself and significantly affect the model’s accuracy, complexity, and training speed. The goal of hyperparameter tuning is to find the combination of these settings that leads to the best performance on a given task (GeeksForGeeks, 2025b).\n",
        "\n",
        "GridSearchCV is a hyperparameter tuning method that exhaustively searches through a predefined grid of hyperparameter values. It trains and evaluates the model on every possible combination of the specified hyperparameters to identify the best performing set. Although GridSearchCV can find the optimal hyperparameters by testing all combinations, it is computationally expensive and slow, especially when dealing with large datasets or many hyperparameters. For example, tuning two parameters with five and four possible values respectively requires training 20 different models, which can be resource-intensive (GeeksForGeeks, 2025b).\n",
        "\n",
        "RandomizedSearchCV improves upon GridSearchCV by randomly sampling hyperparameter combinations from a specified distribution rather than testing every possible combination. This approach allows it to explore a wider range of hyperparameters more efficiently and with fewer iterations. While it may not guarantee finding the absolute best combination, it often finds a near-optimal solution much faster, making it practical for large hyperparameter spaces. RandomizedSearchCV balances exploration and computational efficiency, making it a popular choice for hyperparameter tuning (GeeksForGeeks, 2025b).\n",
        "\n",
        "Bayesian optimization is a more advanced hyperparameter tuning technique that uses a probabilistic model to guide the search for optimal hyperparameters. It builds a surrogate model based on past evaluations to predict the performance of new hyperparameter combinations. By iteratively updating this model, Bayesian optimization intelligently selects hyperparameters that are likely to improve model performance, reducing the number of evaluations needed. Common surrogate models include Gaussian processes and random forests. This method is more sample-efficient but requires an understanding of probabilistic modeling (GeeksForGeeks, 2025b).\n",
        "\n",
        "RandomizedSearchCV is often chosen in practice because it offers a good balance between computational efficiency and performance. Unlike GridSearchCV, it does not exhaustively evaluate all hyperparameter combinations, which saves time and resources. Compared to Bayesian optimization, it is simpler to implement and does not require complex probabilistic modeling knowledge. Although it may not always find the absolute best hyperparameters, it frequently produces near-optimal results quickly, making it a practical and effective choice for many machine learning tasks (GeeksForGeeks, 2025b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0a0b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define categorical feature indices for SMOTENC\n",
        "categorical_columns = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n",
        "categorical_features = [X_train.columns.get_loc(col) for col in categorical_columns]\n",
        "\n",
        "# Define the base pipeline steps: preprocessor, resampler (placeholder), classifier\n",
        "base_steps = [\n",
        "    ('preprocessor', preprocessor),  # preprocessor with best scaler (e.g., RobustScaler)\n",
        "    ('resampler', None),  # placeholder for resampler, will be tuned\n",
        "    ('classifier', AdaBoostClassifier(random_state=42))\n",
        "]\n",
        "\n",
        "# Create imbalanced-learn pipeline\n",
        "imb_pipeline = ImbPipeline(steps=base_steps)\n",
        "\n",
        "# Define parameter distribution including resampler choice and AdaBoost hyperparameters\n",
        "param_distributions = {\n",
        "    'resampler': [\n",
        "        RandomOverSampler(random_state=42),\n",
        "        SMOTE(random_state=42),\n",
        "        SMOTENC(categorical_features=categorical_features, random_state=42),\n",
        "        SMOTEN(random_state=42),\n",
        "        ADASYN(random_state=42),\n",
        "        BorderlineSMOTE(random_state=42),\n",
        "        KMeansSMOTE(random_state=42),\n",
        "        SVMSMOTE(random_state=42),\n",
        "        SMOTEENN(random_state=42),\n",
        "        SMOTETomek(random_state=42),\n",
        "        None  # No resampling\n",
        "    ],\n",
        "    'classifier__estimator': [\n",
        "        DecisionTreeClassifier(max_depth=1),\n",
        "        DecisionTreeClassifier(max_depth=2),\n",
        "        DecisionTreeClassifier(max_depth=3),\n",
        "        LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42)\n",
        "    ],\n",
        "    'classifier__n_estimators': [50, 100, 150, 200, 250],\n",
        "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
        "    'classifier__algorithm': ['SAMME']\n",
        "}\n",
        "\n",
        "# Setup RandomizedSearchCV with recall scoring and stratified CV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=imb_pipeline,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,\n",
        "    scoring='recall',\n",
        "    cv=cv,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV on training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Output best parameters and best recall score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# Update best pipeline with tuned parameters\n",
        "best_tuned_pipeline = random_search.best_estimator_\n",
        "display(best_tuned_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "811936df",
      "metadata": {},
      "source": [
        "#### 4.2.4 Picking The Best Tuning Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46883771",
      "metadata": {},
      "source": [
        "It begins with a preprocessing step using a ColumnTransformer, which applies different transformations to various feature types. Numerical features such as tenure and charges are scaled using a RobustScaler to reduce the impact of outliers. Binary categorical features like dependents and paperless billing are transformed via a custom function to map yes/no values into a suitable numerical format. Ordinal features, such as contract type, are encoded with an OrdinalEncoder to preserve their inherent order, while other categorical features are one-hot encoded to create binary indicators for each category. Any remaining columns not explicitly transformed are passed through unchanged.\n",
        "\n",
        "Following preprocessing, the pipeline addresses class imbalance using a RandomOverSampler. This technique increases the number of samples in the minority class to ensure the classifier receives a balanced dataset, which can improve predictive performance, especially in imbalanced classification problems.\n",
        "\n",
        "The final step of the pipeline is the classification model itself, which is an AdaBoost ensemble classifier. It uses decision stumps (decision trees with a maximum depth of one) as weak learners. The AdaBoost algorithm iteratively trains these weak learners, focusing more on samples that were previously misclassified, and combines their predictions to form a strong overall classifier. The model is configured with a learning rate of 0.01 and includes 250 boosting stages, ensuring a gradual and robust learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2792ab61",
      "metadata": {},
      "source": [
        "### 4.3 Evaluate Best Model on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16bfc789",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict class labels\n",
        "y_pred = best_tuned_pipeline.predict(X_test)\n",
        "\n",
        "# Predict probabilities for positive class (needed for PR curve)\n",
        "y_proba = best_tuned_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "recall_before = best_scores['Mean Score'].loc[6]\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "# Print summary metrics\n",
        "print(f\"Recall Before Hyperparameter Tuning:            {recall_before:.4f}\")\n",
        "print(f\"Recall After Hyperparameter Tuning:            {recall:.4f}\")\n",
        "print(f\"Improvement in Recall: {((recall - recall_before) / recall_before) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb004a95",
      "metadata": {},
      "source": [
        "The best tuned model was evaluated on the test dataset by predicting class labels and the probabilities of the positive class. The recall score was calculated to measure the model’s ability to correctly identify positive instances, achieving a value of 0.9202. This model increases significantly arount 70% from the initial model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f99d9243",
      "metadata": {},
      "source": [
        "### 4.4 Learning Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "953850a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training sizes for learning curve\n",
        "train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "\n",
        "# Calculate learning curve metrics using recall as scoring\n",
        "train_sizes, train_scores, valid_scores = learning_curve(\n",
        "    best_tuned_pipeline,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=cv,\n",
        "    scoring='recall',\n",
        "    train_sizes=train_sizes,\n",
        "    n_jobs=-1,\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Calculate mean and std for training and validation scores\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
        "valid_scores_std = np.std(valid_scores, axis=1)\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training recall')\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,\n",
        "                 alpha=0.2, color='blue')\n",
        "plt.plot(train_sizes, valid_scores_mean, 'o-', color='green', label='Cross-validation recall')\n",
        "plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std,\n",
        "                 alpha=0.2, color='green')\n",
        "\n",
        "plt.title('Learning Curve (Recall)')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('Recall Score')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "545aa5d7",
      "metadata": {},
      "source": [
        "The learning curve plot was generated to show the recall scores for both the training and cross-validation sets as the training size was increased. It is observed that the training recall was maintained at a relatively high level, around 0.88 to 0.89, throughout the range of training sizes. The variability in training recall scores was kept low, as indicated by the narrow shaded area representing the standard deviation.\n",
        "\n",
        "The cross-validation recall was initially lower when smaller training sizes were used, but it was improved and gradually converged toward the training recall as more data was introduced. A wider variability in cross-validation recall was shown at smaller training sizes, but this variability was reduced with increasing training data. The close alignment of training and validation recall scores suggested that the model was well-fitted, with neither overfitting nor underfitting being indicated.\n",
        "\n",
        "Good model performance was demonstrated by the recall scores, which were maintained around 0.88 to 0.89, indicating that false negatives were effectively minimized. The stabilization of recall scores at larger training sizes suggested that additional data beyond a certain point did not result in significant improvements. It was concluded that the model’s capacity had been reached with the current features and algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a94d82a3",
      "metadata": {},
      "source": [
        "### 4.5 Threshold Tuning Using Precision-Recall Curve (Go or No Go ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62546c58",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Precision-Recall curve\n",
        "precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall_vals, precision_vals, color='b', lw=2, label='PR curve')\n",
        "plt.fill_between(recall_vals, precision_vals, alpha=0.2, color='b')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Exclude last point where threshold is not defined (thresholds length is one less than precision/recall)\n",
        "recall_vals_trimmed = recall_vals[:-1]\n",
        "thresholds_trimmed = thresholds\n",
        "\n",
        "# Find threshold that maximizes recall\n",
        "best_idx = np.argmax(recall_vals_trimmed)\n",
        "best_threshold = thresholds_trimmed[best_idx]\n",
        "best_recall = recall_vals_trimmed[best_idx]\n",
        "\n",
        "print(f\"Best threshold by Recall score: {best_threshold:.4f}\")\n",
        "print(f\"Recall score at this threshold: {best_recall:.4f}\")\n",
        "\n",
        "# Apply the best threshold to predicted probabilities to get new predictions\n",
        "y_pred_threshold = (y_proba >= best_threshold).astype(int)\n",
        "\n",
        "# Calculate metrics using the new predictions\n",
        "recall = recall_score(y_test, y_pred_threshold)\n",
        "\n",
        "# Print summary metrics\n",
        "print(f\"Recall:            {recall:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a113f06b",
      "metadata": {},
      "source": [
        "The Precision-Recall curve was plotted to evaluate the trade-off between precision and recall at different classification thresholds. The curve was visualized with recall values on the x-axis and precision values on the y-axis, and the area under the curve was shaded to provide a clearer representation of the relationship between these two metrics (Davis & Goadrich, 2006).\n",
        "\n",
        "A threshold tuning process was conducted by excluding the last recall point where the threshold is undefined. The threshold that maximized recall was identified by finding the index of the highest recall value and selecting the corresponding threshold. This best threshold was found to be approximately 0.1192, at which the recall reached its maximum value of 1.0000.\n",
        "\n",
        "Using this optimized threshold, new predictions were generated by classifying instances with predicted probabilities greater than or equal to the threshold as positive. The recall score was then recalculated based on these predictions, confirming that a perfect recall of 1.0000 was achieved. This indicates that all positive cases were correctly identified when using the tuned threshold.\n",
        "\n",
        "While the recall was maximized, it should be noted that the precision decreased as recall increased, as shown by the curve. This trade-off implies that although no positive cases were missed, the number of false positives likely increased, which may affect the overall model performance depending on the application context. Further evaluation considering precision and other metrics may be required to determine whether this threshold tuning represents a \"Go\" or \"No Go\" decision for deployment.\n",
        "\n",
        "However, this decision will be classified as a \"No Go\" because achieving a perfect recall score of 1.0000 at such a low threshold seems too good to be true. Such an outcome often indicates potential issues such as data leakage, model overfitting, or an unrealistic threshold that may not generalize well to new data. Therefore, caution is advised before adopting this threshold in a real-world setting (Saito & Rehmsmeier, 2015)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49fabd47",
      "metadata": {},
      "source": [
        "## Section 5. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cb0e75",
      "metadata": {},
      "source": [
        "### 5.1 Feature Importance Using LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf298101",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Index of the test instance to explain\n",
        "idx = 0\n",
        "\n",
        "# Extract preprocessor and classifier from the pipeline\n",
        "preprocessor = best_tuned_pipeline.named_steps['preprocessor']\n",
        "classifier = best_tuned_pipeline.named_steps['classifier']\n",
        "\n",
        "# Transform the training data (returns numpy array)\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "\n",
        "# Helper function to get feature names after transformation\n",
        "def get_feature_names(preprocessor):\n",
        "    feature_names = []\n",
        "\n",
        "    # Numeric features (scaled)\n",
        "    numeric_transformer = preprocessor.named_transformers_.get('num')\n",
        "    if numeric_transformer is not None:\n",
        "        feature_names.extend(numeric_cols)\n",
        "\n",
        "    # Binary features (mapped to 0/1)\n",
        "    binary_transformer = preprocessor.named_transformers_.get('binary')\n",
        "    if binary_transformer is not None:\n",
        "        feature_names.extend(binary_cols)\n",
        "\n",
        "    # Ordinal features (Contract)\n",
        "    ordinal_transformer = preprocessor.named_transformers_.get('ordinal')\n",
        "    if ordinal_transformer is not None:\n",
        "        feature_names.extend(['Contract'])\n",
        "\n",
        "    # One-hot encoded categorical features\n",
        "    cat_transformer = preprocessor.named_transformers_.get('cat')\n",
        "    if cat_transformer is not None:\n",
        "        ohe = cat_transformer.named_steps['onehot']\n",
        "        cat_feature_names = ohe.get_feature_names_out(categorical_cols_no_binary)\n",
        "        feature_names.extend(cat_feature_names)\n",
        "\n",
        "    return feature_names\n",
        "\n",
        "# Get feature names after transformation\n",
        "feature_names = get_feature_names(preprocessor)\n",
        "\n",
        "# Define a prediction function for LIME that accepts already preprocessed data\n",
        "def predict_proba_transformed(X):\n",
        "    return classifier.predict_proba(X)\n",
        "\n",
        "# Create LIME explainer with transformed training data\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train_transformed,\n",
        "    feature_names=feature_names,\n",
        "    class_names=['No Churn', 'Churn'],\n",
        "    discretize_continuous=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Transform the test instance to explain\n",
        "X_test_sample_transformed = preprocessor.transform(X_test.iloc[[idx]])\n",
        "\n",
        "# Generate explanation for the selected instance\n",
        "exp = explainer.explain_instance(\n",
        "    data_row=X_test_sample_transformed[0],\n",
        "    predict_fn=predict_proba_transformed,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "# Extract feature contributions (feature name and weight)\n",
        "feature_contributions = exp.as_list()\n",
        "\n",
        "# Separate features and weights, reverse order for plotting (largest on top)\n",
        "features, weights = zip(*feature_contributions)\n",
        "features = features[::-1]\n",
        "weights = weights[::-1]\n",
        "\n",
        "# Define colors: green for positive, red for negative contributions\n",
        "colors = ['green' if w > 0 else 'red' for w in weights]\n",
        "\n",
        "# Display explanation in notebook\n",
        "# exp.show_in_notebook(show_table=True)\n",
        "\n",
        "# Plot horizontal bar chart of feature contributions (uncomment to run)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(features, weights, color=colors)\n",
        "plt.xlabel('Feature Contribution')\n",
        "plt.title(f'LIME Feature Contributions for Test Instance Index {idx}')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157a5104",
      "metadata": {},
      "source": [
        "LIME (Local Interpretable Model-agnostic Explanations) is described as a technique that is used to explain the predictions of any supervised machine learning model by treating it as a black box. It is designed to provide explanations that are locally faithful to the model’s behavior around a specific instance, making the decision-making process more interpretable. Samples are generated around the instance of interest, and a simple, interpretable surrogate model is fitted on these samples to approximate the original model’s predictions. The importance of features contributing to the prediction is then identified, allowing users to understand the reasoning behind complex model outputs in a clear and accessible manner (GeeksforGeeks, 2023).\n",
        "\n",
        "Among the features, the \"Contract\" type has the most significant positive contribution, suggesting that it plays a crucial role in favorably influencing the prediction. Similarly, the \"InternetService_Fiber optic\" feature also contributes positively, indicating that this type of internet service is associated with a higher likelihood of the predicted outcome. In contrast, the \"Tenure\" feature, specifically within the range of 0.00 to 0.55, has a negative contribution, which implies that a tenure in this range reduces the likelihood of the predicted outcome.\n",
        "\n",
        "Other features, such as \"OnlineBackup_No internet service\" and \"DeviceProtection_Yes,\" show minimal to no impact on the prediction. Features like \"TechSupport_Yes\" and \"InternetService_No\" also contribute very little, indicating that they do not significantly sway the model's decision for this instance. Overall, the plot highlights that the model's prediction is heavily influenced by the contract type and internet service, while the tenure within a specific range has a detrimental effect. This interpretation aids in understanding the model's behavior and the importance of each feature in making predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c13cee6",
      "metadata": {},
      "source": [
        "### 5.2 Rule Based vs Machine Learning Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73beebd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def predict_churn_qcut(row):\n",
        "    # Rule 1: Month-to-month contract and low tenure (lowest quartile)\n",
        "    if row['Contract'] == 'Month-to-month' and row['tenure_bin'] == 1:\n",
        "        return 1\n",
        "    \n",
        "    # Rule 2: Fiber optic with high monthly charges (highest quartile)\n",
        "    if row['InternetService'] == 'Fiber optic' and row['MonthlyCharges_bin'] == 4:\n",
        "        return 1\n",
        "    \n",
        "    # Rule 3: Paperless billing with low to medium tenure (q1 or q2)\n",
        "    if row['PaperlessBilling'] == 'Yes' and row['tenure_bin'] in [1, 2]:\n",
        "        return 1\n",
        "    \n",
        "    # Rule 4: Lack of protective services and low tenure\n",
        "    if (row['OnlineSecurity'] == 'No' or row['DeviceProtection'] == 'No' or row['TechSupport'] == 'No') and row['tenure_bin'] == 1:\n",
        "        return 1\n",
        "    \n",
        "    # Rule 5: Long tenure (highest quartile) and longer contracts predict no churn\n",
        "    if row['tenure_bin'] == 4 and row['Contract'] in ['One year', 'Two year']:\n",
        "        return 0\n",
        "    \n",
        "    # Default: No churn\n",
        "    return 0\n",
        "\n",
        "X_test_rule = X_test.copy()\n",
        "y_test_rule = y_test.copy()\n",
        "X_test_rule['Churn'] = X_test_rule.apply(predict_churn_qcut, axis=1)\n",
        "y_pred_rule = X_test_rule['Churn']\n",
        "\n",
        "recall_score_rule = recall_score(y_test_rule, y_pred_rule)\n",
        "recall_score_ml = recall_score(y_test, y_pred)\n",
        "recall_differences = abs(recall_score_rule - recall_score_ml)\n",
        "\n",
        "print(f\"Recall Score Rule: {recall_score_rule:.4f}\")\n",
        "print(f\"Recall Score ML: {recall_score_ml:.4f}\")\n",
        "print(f\"Recall Difference: {recall_differences:.4f}\")\n",
        "\n",
        "cm1 = confusion_matrix(y_test_rule, y_pred_rule)\n",
        "cm2 = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Labels in confusion matrix order\n",
        "labels = np.array([['TN', 'FP'],\n",
        "                   ['FN', 'TP']])\n",
        "\n",
        "# Prepare annotations for cm1\n",
        "annot1 = np.empty_like(cm1).astype(str)\n",
        "for i in range(cm1.shape[0]):\n",
        "    for j in range(cm1.shape[1]):\n",
        "        annot1[i, j] = f\"{labels[i, j]}\\n{cm1[i, j]}\"\n",
        "\n",
        "# Prepare annotations for cm2\n",
        "annot2 = np.empty_like(cm2).astype(str)\n",
        "for i in range(cm2.shape[0]):\n",
        "    for j in range(cm2.shape[1]):\n",
        "        annot2[i, j] = f\"{labels[i, j]}\\n{cm2[i, j]}\"\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "sns.heatmap(cm1, annot=annot1, fmt='', cmap='Blues', cbar=False,\n",
        "            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'], ax=axes[0])\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title('Confusion Matrix - Rule-based Prediction')\n",
        "\n",
        "sns.heatmap(cm2, annot=annot2, fmt='', cmap='Blues', cbar=False,\n",
        "            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'], ax=axes[1])\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "axes[1].set_title('Confusion Matrix - Model Prediction')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe293d63",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given data\n",
        "CAC = 200  # Customer Acquisition Cost per customer ($)\n",
        "retention_cost = 50  # Retention Cost per customer ($)\n",
        "period_months = 12  # Analysis period in months\n",
        "\n",
        "# Extract FN and FP from confusion matrices\n",
        "# cm1 and cm2 are confusion matrices from your previous code\n",
        "TN_rule, FP_rule = cm1[0, 0], cm1[0, 1]\n",
        "FN_rule, TP_rule = cm1[1, 0], cm1[1, 1]\n",
        "\n",
        "TN_ml, FP_ml = cm2[0, 0], cm2[0, 1]\n",
        "FN_ml, TP_ml = cm2[1, 0], cm2[1, 1]\n",
        "\n",
        "# Rule-Based Scenario\n",
        "loss_per_FN_rule = CAC  # Loss per FN customer ($)\n",
        "cost_per_FP_rule = retention_cost  # Cost per FP customer ($)\n",
        "\n",
        "# Machine Learning Scenario\n",
        "loss_per_FN_ml = CAC  # Loss per FN customer ($)\n",
        "cost_per_FP_ml = retention_cost  # Cost per FP customer ($)\n",
        "\n",
        "# Calculations for Rule-Based\n",
        "loss_FN_rule = FN_rule * loss_per_FN_rule\n",
        "cost_FP_rule = FP_rule * cost_per_FP_rule\n",
        "total_monthly_loss_rule = loss_FN_rule + cost_FP_rule\n",
        "total_annual_loss_rule = total_monthly_loss_rule * period_months\n",
        "\n",
        "# Calculations for Machine Learning\n",
        "loss_FN_ml = FN_ml * loss_per_FN_ml\n",
        "cost_FP_ml = FP_ml * cost_per_FP_ml\n",
        "total_monthly_loss_ml = loss_FN_ml + cost_FP_ml\n",
        "total_annual_loss_ml = total_monthly_loss_ml * period_months\n",
        "\n",
        "# Print results\n",
        "print(\"Rule-Based Scenario:\")\n",
        "print(f\"  Loss from FN: ${loss_FN_rule:,}\")\n",
        "print(f\"  Cost from FP: ${cost_FP_rule:,}\")\n",
        "print(f\"  Total Monthly Loss: ${total_monthly_loss_rule:,}\")\n",
        "print(f\"  Total Annual Loss: ${total_annual_loss_rule:,}\\n\")\n",
        "\n",
        "print(\"Machine Learning Scenario:\")\n",
        "print(f\"  Loss from FN: ${loss_FN_ml:,}\")\n",
        "print(f\"  Cost from FP: ${cost_FP_ml:,}\")\n",
        "print(f\"  Total Monthly Loss: ${total_monthly_loss_ml:,}\")\n",
        "print(f\"  Total Annual Loss: ${total_annual_loss_ml:,}\")\n",
        "\n",
        "print()\n",
        "print(f\"Differences: ${abs(total_annual_loss_ml - total_annual_loss_rule):,}\")\n",
        "print(f\"Percentages savings: {((total_annual_loss_rule - total_annual_loss_ml) / total_annual_loss_rule) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0c2bce",
      "metadata": {},
      "source": [
        "The Machine Learning Scenario significantly increases the recall score around 9%. It also significantly reduces both the loss from False Negatives and the cost from False Positives compared to the Rule-Based Scenario. This leads to a substantial decrease in total monthly and annual losses. Implementing the Machine Learning approach can therefore be seen as a more cost-effective strategy, saving nearly $80,000 annually or about a quarter of the total losses experienced under the Rule-Based system. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39cd8b3",
      "metadata": {},
      "source": [
        "## Section 6. Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc0c8283",
      "metadata": {},
      "source": [
        "### 6.1 Save Model Using Joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f3f2fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the relative path to save the model\n",
        "model_save_path = Path(\"models/best_tuned_pipeline.joblib\")\n",
        "\n",
        "# Create directory if it does not exist\n",
        "model_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save the best tuned pipeline\n",
        "joblib.dump(best_tuned_pipeline, model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156397ea",
      "metadata": {},
      "source": [
        "Joblib is used to save and load Python objects easily and quickly. It is designed to handle big data like large arrays and machine learning models. The library is made to work well with Python programs and helps store information on the computer’s disk. It is often used to keep models safe after they are trained so they can be used later without retraining. Joblib also supports running tasks in parallel to make programs faster (Joblib, 2025).\n",
        "\n",
        "The process of saving a trained model is demonstrated by defining a relative file path where the model will be stored. The necessary directory is created if it does not already exist, ensuring that the save operation can proceed without errors. The model, specifically the best tuned pipeline, is then saved to the specified location using the joblib library. A confirmation message is printed to inform the user that the model has been successfully saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Model Limitations\n",
        "\n",
        "The model’s limitations include dependence on the quality and representativeness of the training data, which may contain biases affecting prediction accuracy. Its ability to identify complex patterns or rare events is restricted by the chosen features. Additionally, the model may suffer from overfitting or underfitting if not properly tuned. Changes in customer behavior or external factors over time are not captured unless the model is regularly updated. Predictions are based solely on historical data, limiting the ability to anticipate unforeseen events. Finally, the model’s interpretability may be limited, and the evaluation metrics used might not fully reflect all aspects of its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7. Conclusion and Recommendations\n",
        "\n",
        "### 7.1 Summary\n",
        "\n",
        "The business context and objectives for the telecom customer churn prediction project. The challenge of customer churn, which impacts revenue and growth, is addressed by developing predictive models to identify at-risk customers. Key factors influencing churn are examined, and actionable insights are aimed to be provided for improving retention. A structured analytical approach is planned, including model evaluation based on recall, with success criteria established to ensure high performance and interpretability.\n",
        "\n",
        "The dataset is explored to understand its structure and quality. Data completeness is confirmed with no missing values found, while duplicated rows are noted but retained after careful consideration. Data types are adjusted for effective analysis, converting object columns to categorical types. Exploratory data analysis reveals important patterns in customer demographics, service usage, and churn distribution. Relationships among features are examined using visualizations and correlation analyses to inform feature selection and preprocessing strategies.\n",
        "\n",
        "The data preparation steps undertaken to ready the dataset for modeling. A new feature representing total charges is engineered, and the target variable is encoded into a binary format. The dataset is split into training and testing sets using stratified sampling to preserve class distribution. Feature types are identified, and preprocessing pipelines are constructed to apply appropriate transformations, including scaling, encoding, and mapping of categorical and binary variables. This setup ensures consistent and efficient data handling during model training.\n",
        "\n",
        "Multiple classification models are initialized and evaluated using cross-validation with recall as the scoring metric. Various scalers are tested to identify the most effective preprocessing approach. An ensemble stacking classifier is included among the models to leverage combined strengths. Hyperparameter tuning is performed with a focus on balancing class distributions using advanced resampling techniques and optimizing AdaBoost parameters. The best-performing tuned model is selected based on recall performance.\n",
        "\n",
        "The evaluation of the best model on test data. Predictions are generated, and recall is calculated to assess model effectiveness, achieving high recall scores. Learning curves demonstrate stable performance without overfitting. Threshold tuning using precision-recall curves is conducted but deemed unsuitable due to potential overfitting concerns. Model interpretability is enhanced using LIME explanations, highlighting key features influencing predictions. Confusion matrix visualization provides insight into classification errors, and churn rate comparisons reveal discrepancies between predicted and actual churn rates.\n",
        "\n",
        "The deployment process, including saving the final model using joblib for future use. Instructions for loading the saved model and preparing new customer data for prediction are provided, with example scenarios illustrating model application. Limitations of the model are acknowledged, particularly the tendency to overestimate churn due to prioritizing recall. Recommendations for business actions and model improvements are outlined to guide practical implementation and ongoing refinement.\n",
        "\n",
        "### 7.2 Conclusion\n",
        "- Contract has the strongest positive contribution to churn prediction — customers with certain contract types are more likely to churn.\n",
        "- InternetService_Fiber optic also positively contribute to churn risk.\n",
        "- Tenure has a negative contribution — shorter tenure customers are more likely to churn.\n",
        "- The Machine Learning Scenario significantly increases recall score around 9% and reduces False Negatives and False Positives losses, saving nearly $80,000 annually, a quarter of the total losses under the Rule-Based system.\n",
        "\n",
        "### 7.3 Business Recommendations\n",
        "To reduce the churn, the company can consider the following business strategies:\n",
        "- Contract Optimization:\n",
        "    + Promote Favorable Contracts: Focus marketing efforts on promoting contract types that positively influence customer retention and satisfaction. Consider offering incentives for customers to choose these contracts.\n",
        "- Internet Service Offerings:\n",
        "    + Enhance Fiber Optic Services: Since the \"InternetService_Fiber optic\" feature positively impacts predictions, invest in expanding fiber optic infrastructure and promoting its benefits to attract more customers.\n",
        "- Customer Retention Strategies:\n",
        "    + Address Tenure Concerns: Develop targeted retention strategies for customers with tenures in the 0.00 to 0.55 range. This could include personalized outreach, loyalty programs, or special offers to encourage them to stay.\n",
        "- Feature Awareness Campaigns:\n",
        "    + Educate Customers: Create campaigns to inform customers about the benefits of additional features, such as online backups and device protection, which may not currently be perceived as valuable.\n",
        "- Customer Feedback Loop:\n",
        "    + Gather Feedback: Implement mechanisms to gather customer feedback on service features and contracts. Use this data to refine offerings and address any pain points that may lead to churn.\n",
        "- Analyze Customer Segments:\n",
        "    + Segment Analysis: Conduct further analysis on customer segments to understand the specific needs and behaviors of different groups, allowing for more tailored marketing and service offerings.\n",
        "- Retention Metrics Monitoring:\n",
        "    + Track Key Metrics: Regularly monitor key metrics related to contract types, internet service usage, and customer tenure to identify trends and adjust strategies accordingly.\n",
        "\n",
        "### 7.4 Model Recommendations\n",
        "To improve machine learning models for churn prediction, the company can consider the following strategies:\n",
        "\n",
        "- Balance Precision and Recall:\n",
        "    + Instead of focusing solely on maximizing recall, the company can tune the model to achieve a better balance between precision and recall. This can be done by adjusting the classification threshold or using evaluation metrics like the F1-score that consider both precision and recall.\n",
        "- Regular Model Retraining:\n",
        "    + Customer behavior and market conditions change over time. Regularly retraining the model with new data ensures that it adapts to recent trends and maintains accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "Batista, G. E. A. P. A., Prati, R. C., & Monard, M. C. (2004). A study of the behavior of several methods for balancing machine learning training data. *ACM SIGKDD Explorations Newsletter, 6*(1), 20–29. https://doi.org/10.1145/1007730.1007735\n",
        "\n",
        "Belcic, I. (2024, October 15). *What is classification in machine learning?* https://www.ibm.com/think/topics/classification-machine-learning\n",
        "\n",
        "Bhatnagar, A., & Srivastava, S. (2025). Customer churn analysis in telecom. *Computing Online.* https://computingonline.net/computing/article/view/3873\n",
        "\n",
        "BlastChar. (2018). *Telco customer churn prediction* [Dataset]. Kaggle. https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
        "\n",
        "Britto, F. (2024, September 9). Churn in telecom: What it is and its impacts. *SYDLE.* https://www.sydle.com/blog/churn-telecom-66b4c9e8b2713612fa17da85\n",
        "\n",
        "Davis, J., & Goadrich, M. (2006). The relationship between precision-recall and ROC curves. In *Proceedings of the 23rd International Conference on Machine Learning (ICML ’06)* (pp. 233–240). ACM. https://doi.org/10.1145/1143844.1143874\n",
        "\n",
        "Fawcett, T., & Niculescu-Mizil, A. (2007). Pavlovian precision: Predicting good probabilities. In *Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI)* (pp. 179–186). AUAI Press.\n",
        "\n",
        "Galarnyk, M. (2025, February 3). *Train test split: What it means and how to use it.* Built In. https://builtin.com/data-science/train-test-split\n",
        "\n",
        "GeeksforGeeks. (2023, April 11). *Explainable AI (XAI) using LIME.* https://www.geeksforgeeks.org/introduction-to-explainable-aixai-using-lime/\n",
        "\n",
        "GeeksforGeeks. (2024, September 4). *Top 6 machine learning classification algorithms.* https://www.geeksforgeeks.org/top-6-machine-learning-algorithms-for-classification/\n",
        "\n",
        "GeeksforGeeks. (2025a, January 23). *Ensemble learning.* https://www.geeksforgeeks.org/a-comprehensive-guide-to-ensemble-learning/\n",
        "\n",
        "GeeksforGeeks. (2025b, March 11). Hyperparameter tuning. https://www.geeksforgeeks.org/hyperparameter-tuning/\n",
        "\n",
        "Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On calibration of modern neural networks. In *Proceedings of the 34th International Conference on Machine Learning (ICML)* (Vol. 70, pp. 1321–1330). PMLR. https://arxiv.org/abs/1706.04599\n",
        "\n",
        "He, H., & Garcia, E. A. (2009). Learning from imbalanced data. *IEEE Transactions on Knowledge and Data Engineering, 21*(9), 1263–1284. https://doi.org/10.1109/TKDE.2008.239\n",
        "\n",
        "Hermawan, A., Jayanti, N. R., Tabaruk, Z., Triadi, F. L. Y., Saputra, A., & Syachrudin, M. R. H. (2024). Membangun model prediksi churn pelanggan yang akurat (Studi kasus tentang TELCO Company). *Merkurius: Jurnal Riset Sistem Informasi dan Teknik Informatika,* 2(6), 67–81. https://doi.org/10.61132/merkurius.v2i6.398\n",
        "\n",
        "IBM. (n.d.). *What are naïve Bayes classifiers?* https://www.ibm.com/think/topics/naive-bayes\n",
        "\n",
        "Idris, N. L. (2024, July 31). *Streamlining data preprocessing and cleaning in a machine learning pipeline.* Medium. https://medium.com/@nafisaidris413/streamlining-data-preprocessing-and-cleaning-in-a-machine-learning-pipeline-a6e602de2e57\n",
        "\n",
        "imbalanced-learn developers. (2024a). Over-sampling methods. *imbalanced-learn.* https://imbalanced-learn.org/stable/references/over_sampling.html\n",
        "\n",
        "imbalanced-learn developers. (2024b). Combination of over- and under-sampling methods. *imbalanced-learn.* https://imbalanced-learn.org/stable/references/combine.html\n",
        "\n",
        "Imhoff, C., Galemmo, N., & Geiger, J. G. (2003). *Mastering data warehouse design: Relational and dimensional techniques*. John Wiley & Sons.\n",
        "\n",
        "Investopedia Team. (2024, March 21). Churn rate: What it means, examples, and calculations. *Investopedia.* https://www.investopedia.com/terms/c/churnrate.asp\n",
        "\n",
        "IQ Team. (2025, February 23). Top 17 customer churn datasets and projects (updated for 2025). *Interview Query.* https://www.interviewquery.com/p/customer-churn-datasets\n",
        "\n",
        "Jain, A. K. (2010). Data clustering: 50 years beyond K-means. *Pattern Recognition Letters, 31*(8), 651–666. https://doi.org/10.1016/j.patrec.2009.09.011\n",
        "\n",
        "Kumari, D., Singh, S. K., Katira, S. S., Srinivas, I. V., & Salunkhe, U. (2025). Telecom customer churn forecasting using machine learning: A data-driven predictive framework. *Metallurgical and Materials Engineering, 31*(4), 922–929. https://doi.org/10.63278/1536\n",
        "\n",
        "Liu, H., Hussain, F., Tan, C. L., & Dash, M. (2002). *Discretization: An enabling technique. Data Mining and Knowledge Discovery*, 6(4), 393–423.\n",
        "\n",
        "Liu, H., Motoda, H., Setiono, R., & Zhao, Z. (2020). Feature selection: An ever-evolving frontier in data mining. *Journal of Machine Learning Research, 21*(1), 1–50.\n",
        "\n",
        "Luck, I. (2023, April 4). What is customer churn? Complete meaning & guide. *CustomerGauge.* https://customergauge.com/customer-churn\n",
        "\n",
        "Mahesh, T. R., Kumar, V., Dhilip, K., Geman, O., Margala, M., & Guduri, M. (2023). The stratified K-folds cross-validation and class-balancing methods with high-performance ensemble classifiers for breast cancer classification. *Healthcare Analytics, 4*, 100247. https://doi.org/10.1016/j.health.2023.100247\n",
        "\n",
        "McKinney, W. (2017). *Python for data analysis: Data wrangling with pandas, NumPy, and IPython* (2nd ed.). O'Reilly Media.\n",
        "\n",
        "Mukaka, M. M. (2012). A guide to appropriate use of correlation coefficient in medical research. *Malawi Medical Journal, 24*(3), 69–71. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/\n",
        "\n",
        "Murel, J., & Kavlakoglu, E. (2024, March 18). *What is ensemble learning?* https://www.ibm.com/think/topics/ensemble-learning\n",
        "\n",
        "Niculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning. In *Proceedings of the 22nd International Conference on Machine Learning (ICML)* (pp. 625–632). ACM. https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf\n",
        "\n",
        "Saito, T., & Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. *PLOS ONE, 10*(3), e0118432. https://doi.org/10.1371/journal.pone.0118432\n",
        "\n",
        "Schober, P., Boer, C., & Schwarte, L. A. (2018). Correlation coefficients: Appropriate use and interpretation. *Anesthesia & Analgesia, 126*(5), 1763–1768. https://doi.org/10.1213/ANE.0000000000002864\n",
        "\n",
        "scikit-learn developers. (2025). sklearn.ensemble.AdaBoostClassifier. *Scikit-learn.* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
        "\n",
        "O’Brien, K., & Downie, A. (2024, September 9). What is customer churn? *IBM.* https://www.ibm.com/think/topics/customer-churn\n",
        "\n",
        "Oracle Community. (2024). OAC - Table visualization in DV duplicate rows, but LSQL is correct. *Oracle Community.* https://community.oracle.com/products/oracleanalytics/discussion/21184/oac-table-visualization-in-dv-duplicate-rows-but-lsql-is-correct\n",
        "\n",
        "Slingerland, C. (2023, October 23). *How to calculate customer retention cost: The hidden SaaS metric.* CloudZero. https://www.cloudzero.com/blog/customer-retention-cost/\n",
        "\n",
        "VanderPlas, J. (2016). *Python data science handbook: Essential tools for working with data*. O'Reilly Media.\n",
        "\n",
        "Wickham, H., & Grolemund, G. (2017). *R for data science: Import, tidy, transform, visualize, and model data*. O'Reilly Media.\n",
        "\n",
        "Yoga, L. (2024). *Building a Customer Churn Prediction Model for WirelessUS* [Jupyter Notebook]. GitHub. https://github.com/ABCDullahh/TELCO-Customer-Churn-Model-Prediction/blob/main/Capstone%20Modul%203.ipynb"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
